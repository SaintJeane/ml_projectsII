{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- This notebook demonstrates the upload for the multimodal RAG system to Hugging Face Space programmatically from the notebook setting."
      ],
      "metadata": {
        "id": "m2q60xcAK8zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q PyMuPDF gradio faiss-cpu\n",
        "# !pip install -qU transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "pUl9HDnlN9pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "File structure of the RAG system that will be published to Hugging Face Space:\n",
        "```\n",
        "  setup/\n",
        "    |-- multimodal_rag/\n",
        "        |-- app.py\n",
        "        |-- main.py\n",
        "        |-- model_setup.py\n",
        "        |-- utils.py\n",
        "        |-- README.md\n",
        "        |-- requirements.txt\n",
        "        |-- cache/\n",
        "```\n",
        "\n",
        "- `app.py` - contains the gradio setup and interface\n",
        "- `main.py` - for PDF processing, chunking, embeddings, semantic search\n",
        "- `model_setup.py` - loads Gemma3, processor and embedding model.\n",
        "- `utils.py` - helper functiions for FAISS, caching and cleanup.\n",
        "- `cache/` - a directory that is autogenerated per-PDF FAISS and chunk files.\n",
        "- `README.md` - a little documnentation about the system, also includes the yaml block at the top with configuration details which is required when publishing to hugging face space.\n",
        "- `requirements.txt` - a txt file with the required packages to be installed for running the system listed."
      ],
      "metadata": {
        "id": "i_ZdSmxYVO_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***NOTE:*** For the Hugging Face version that is uploaded, the model is not going to be set to use GPU because for the free tier Hugging Face Space, CPU is only available."
      ],
      "metadata": {
        "id": "UeMJ7H3xKUDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create directory path for the files"
      ],
      "metadata": {
        "id": "vtr91PYKX0nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "dir_name = Path(\"setup/multimodal_rag/\")\n",
        "dir_name.mkdir(exist_ok=True, parents=True)"
      ],
      "metadata": {
        "id": "yBjYXvDxXy7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `utils.py`"
      ],
      "metadata": {
        "id": "zNfCkWnCXqAp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbYmf6zgVD6a",
        "outputId": "468ea0af-c986-4868-bac9-1ec73d215496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting setup/multimodal_rag/utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile setup/multimodal_rag/utils.py\n",
        "\"\"\"Contains helper functions that are used in the RAG pipeline.\"\"\"\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import json\n",
        "import torch\n",
        "import shutil\n",
        "from typing import List, Dict\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "def save_cache(data: List[Dict], filepath: str) -> None:\n",
        "  \"\"\"Saving the chunks and the embeddings for easy retrieval in .json format\"\"\"\n",
        "  try:\n",
        "    with open(filepath, 'w', encoding='utf-8') as f:\n",
        "      json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "  except Exception as e:\n",
        "    print(f\"Failed to save cache to {filepath}: {e}\")\n",
        "\n",
        "def load_cache(filepath: str) -> List[Dict]:\n",
        "  \"\"\"Loading the saved cache\"\"\"\n",
        "  if os.path.exists(filepath):\n",
        "    try:\n",
        "      with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "    except Exception as e:\n",
        "      print(f\"Failed to load cache from {filepath}: {e}\")\n",
        "  return []\n",
        "\n",
        "\n",
        "# Vector Store Helper Functions using IndexFlatIP (for semantic search)\n",
        "def init_faiss_indexflatip(embedding_dim:int) -> faiss.IndexFlatIP:\n",
        "  index = faiss.IndexFlatIP(embedding_dim)\n",
        "  return index\n",
        "\n",
        "def add_embeddings_to_index(index, embeddings: np.ndarray):\n",
        "  if embeddings.size > 0: # Embedding array is not empty\n",
        "    index.add(embeddings.astype(np.float32))\n",
        "\n",
        "def search_faiss_index(index, query_embedding: np.ndarray, k: int = 5):\n",
        "  # Ensure query_embedding is 2D\n",
        "    if query_embedding.ndim == 1:\n",
        "      query_embedding = query_embedding.reshape(1, -1)\n",
        "    distances, indices = index.search(query_embedding.astype(np.float32), k)\n",
        "    return distances, indices\n",
        "\n",
        "def save_faiss_index(index, filepath: str):\n",
        "  faiss.write_index(index, filepath)\n",
        "\n",
        "def load_faiss_index(filepath: str):\n",
        "  return faiss.read_index(filepath)\n",
        "\n",
        "\n",
        "# Deleting extracted images directory after captioning\n",
        "def cleanup_images(image_dir: str):\n",
        "  try:\n",
        "    shutil.rmtree(image_dir)\n",
        "    print(f\"[INFO] Cleaned up extracted images directory: {image_dir}\")\n",
        "  except Exception as e:\n",
        "    print(f\"[WARNING] Failed to delete some images in {image_dir}: {e}\")\n",
        "\n",
        "# Just being agnostic because my space may only be using CPU but why not?\n",
        "def clear_gpu_cache():\n",
        "  \"\"\"Clear GPU cache and run garbage collection(saving on memory).\"\"\"\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "  gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `model_setup.py`"
      ],
      "metadata": {
        "id": "6_ODEGK8aReN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up for the Hugging Face authorization within the notebook\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN_2\")\n",
        "login(HF_TOKEN)"
      ],
      "metadata": {
        "id": "zFIonxDycfEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile setup/multimodal_rag/model_setup.py\n",
        "\"\"\"loading the models to be used by the Mulltimodal RAG system.\"\"\"\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoProcessor, Gemma3ForConditionalGeneration, BitsAndBytesConfig\n",
        "# from accelerate import disk_offload\n",
        "from utils import clear_gpu_cache\n",
        "\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Embedding model\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Gemma3 quantization config\n",
        "model_name = \"google/gemma-3-4b-it\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    # llm_int8_enable_fp32_cpu_offload=True  # Allow offloading\n",
        ")\n",
        "\n",
        "# Load Gemma3\n",
        "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"cpu\", # Explicitly avoid meta tensors\n",
        "    # quantization_config=bnb_config,\n",
        "    # low_cpu_mem_usage=False, # To avoid lazy meta tensors\n",
        "    # attn_implementation=\"sdpa\"\n",
        ")\n",
        "# disk_offload(model=model, offload_dir=\"offload\")\n",
        "model.to(\"cpu\") # Explicitly load to CPU\n",
        "model.eval()\n",
        "\n",
        "# Processor\n",
        "processor = AutoProcessor.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "clear_gpu_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr-JGfSBaLW_",
        "outputId": "54639215-d6ee-4349-fc01-70cd93c611df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting setup/multimodal_rag/model_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `main.py`"
      ],
      "metadata": {
        "id": "QZkMt0E1smjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile setup/multimodal_rag/main.py\n",
        "\"\"\"Main Mulitmodal-RAG pipeline script.\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import fitz #PyMuPDF\n",
        "import faiss\n",
        "import re\n",
        "import gc\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from typing import List, Dict, Tuple\n",
        "from PIL import Image\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import TextIteratorStreamer\n",
        "\n",
        "from utils import (\n",
        "    save_cache, load_cache,\n",
        "    init_faiss_indexflatip, add_embeddings_to_index,\n",
        "    search_faiss_index, save_faiss_index, load_faiss_index, cleanup_images, clear_gpu_cache\n",
        ")\n",
        "\n",
        "from model_setup import embedding_model, model, processor\n",
        "\n",
        "torch.set_num_threads(4) # Just being agnostic\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Function to extract text and images from each page of the PDF\n",
        "# This function uses PyMuPDF (fitz) to extract text and images from each page\n",
        "def extract_pages_text_and_images(pdf_path, image_dir):\n",
        "  \"\"\"Extract text and images page-wise\"\"\"\n",
        "  doc = fitz.open(pdf_path)\n",
        "  os.makedirs(image_dir, exist_ok=True)\n",
        "\n",
        "  page_texts = []\n",
        "  page_images = []\n",
        "\n",
        "  for page_num in range(len(doc)):\n",
        "    page = doc.load_page(page_num)\n",
        "    text = page.get_text()\n",
        "\n",
        "    # Store all images on this page, store their paths\n",
        "    images = []\n",
        "    for img_index, img in enumerate(page.get_images(full=True)):\n",
        "      xref = img[0]\n",
        "      base_image = doc.extract_image(xref)\n",
        "      image_bytes = base_image[\"image\"]\n",
        "      image_ext = base_image[\"ext\"]\n",
        "      image_filename = f\"page_{page_num + 1}_img_{img_index}.{image_ext}\"\n",
        "      image_path = os.path.join(image_dir, image_filename)\n",
        "      with open(image_path, \"wb\") as img_file:\n",
        "        img_file.write(image_bytes)\n",
        "      images.append(image_path)\n",
        "\n",
        "    page_texts.append(text)\n",
        "    page_images.append(images)\n",
        "\n",
        "  if doc: doc.close()\n",
        "  return page_texts, page_images\n",
        "\n",
        "# Generate image descriptions using the Gemma3 model\n",
        "# This function will be called in parallel for each page's images\n",
        "def generate_image_descriptions(image_paths):\n",
        "  \"\"\"Generate images and tables descriptions as texts.\"\"\"\n",
        "  captions = []\n",
        "  if not processor or not model:\n",
        "    print(\"[ERROR] Model or Processor not loaded. Cannot generate image descriptions.\")\n",
        "    return []\n",
        "  for image_path in image_paths:\n",
        "    raw_image = Image.open(image_path)\n",
        "    if raw_image.mode != \"RGB\":\n",
        "      image = raw_image.convert(\"RGB\")\n",
        "    else:\n",
        "      image = raw_image\n",
        "    width, height = image.size\n",
        "    if width < 32 or height < 32: # Filtering out smaller images that may disrupt the process\n",
        "      continue\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "            {\"type\": \"image\", \"image\": image},\n",
        "            {\"type\": \"text\", \"text\": \"Describe the factual content visible in the image. Be concise and accurate as the descriptions will be used for retrieval.\"}\n",
        "        ]}\n",
        "    ]\n",
        "    try:\n",
        "      inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True,\n",
        "                                             return_dict=True, return_tensors=\"pt\").to(\"cpu\", dtype=torch.bfloat16)\n",
        "      input_len = inputs[\"input_ids\"].shape[-1] # To get rid of the prompt echo\n",
        "      with torch.inference_mode():\n",
        "        generated_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            do_sample=False,\n",
        "            cache_implementation=\"offloaded_static\"\n",
        "            )\n",
        "\n",
        "        raw = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        caption = clean_caption(raw)\n",
        "        captions.append({\"image_path\": image_path, \"caption\": caption})\n",
        "    except Exception as e:\n",
        "      print(f\"[‚ö†Ô∏è ERROR]: Failed to generate caption for image {image_path}: {e}\")\n",
        "      captions.append({\"image_path\": image_path, \"caption\": \"<---image---> (Captioning failed)\"}) # Add a placeholder caption\n",
        "      continue\n",
        "    finally:\n",
        "      gc.collect()\n",
        "      clear_gpu_cache()\n",
        "  return captions\n",
        "\n",
        "# Cleaning the captions from the extracted images\n",
        "# Regex: match everything from \"model\\n\" up through first double-newline after \"sent:\"\n",
        "prefix_re = re.compile(\n",
        "    r\"model\\s*\\n.*?\\bsent:\\s*\\n\\n\",\n",
        "    flags=re.IGNORECASE | re.DOTALL,\n",
        ")\n",
        "\n",
        "def clean_caption(raw: str) -> str:\n",
        "  # 1. Strip off the prompt/header by splitting once.\n",
        "  parts = prefix_re.split(raw.strip(), maxsplit=1)\n",
        "  if len(parts) == 2:\n",
        "    return parts[1].strip()\n",
        "\n",
        "  # 2. Fallback: if the caption begins with ** (bold header), return from there.\n",
        "  bold_index = raw.find(\"**\")\n",
        "  if bold_index >= 0:\n",
        "    return raw[bold_index:].strip()\n",
        "\n",
        "  # 3. Last resort: return everything except the first paragraph.\n",
        "  paras = raw.strip().split('\\n\\n', 1)\n",
        "  return paras[-1].strip()  # might still include some leading noise\n",
        "\n",
        "# Generate captions for all images on each page\n",
        "def generate_captions_per_page(page_image_paths_list):\n",
        "  \"\"\"\"Generate captions per page's images\"\"\"\n",
        "  page_captions = []\n",
        "  for image_paths in page_image_paths_list:\n",
        "    captions = generate_image_descriptions(image_paths)\n",
        "    # Extract the 'caption' strings only\n",
        "    captions_texts = [cap['caption'] for cap in captions]\n",
        "    page_captions.append(captions_texts)\n",
        "  return page_captions\n",
        "\n",
        "# Merge text and captions for each page\n",
        "# This function combines the text and captions for each page into a single string\n",
        "def merge_text_and_captions(page_texts, page_captions):\n",
        "  \"\"\"Merge text, image captions and table descriptions per page\"\"\"\n",
        "  combined_pages = []\n",
        "  for page_num, (text, captions) in enumerate(zip(page_texts, page_captions), 1):\n",
        "    page_content = text.strip() + \"\\n\\n\"\n",
        "    for cap in captions:\n",
        "      page_content += f\"[Image Description]: {cap}\\n\\n\"\n",
        "    combined_pages.append(page_content)\n",
        "  return combined_pages\n",
        "\n",
        "# Chunk the merged pages into smaller text chunks with metadata\n",
        "# This function splits the combined text of each page into smaller chunks\n",
        "def chunk_text_with_metadata(merged_pages):\n",
        "  \"\"\"\n",
        "  Given a list of pages (strings) with combined text and image captions,\n",
        "  split each page's content into chunks, attach metadata, and collect all chunks.\n",
        "\n",
        "  Args:\n",
        "      merged_pages (List[str]): List where each item is the content (text + captions) of a single page.\n",
        "\n",
        "  Returns:\n",
        "      List[dict]: List of chunked dicts with keys: content, page, chunk_id, type\n",
        "  \"\"\"\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"], # Recursive splitting separators, from paragraphs to words\n",
        "      chunk_size =1000,\n",
        "      chunk_overlap =200,\n",
        "      add_start_index=True\n",
        "  )\n",
        "\n",
        "  all_chunks = []\n",
        "  chunk_global_id = 0\n",
        "\n",
        "  for page_num, page_content in enumerate(merged_pages, start=1):\n",
        "    # Split page content into chunks\n",
        "    page_chunks = text_splitter.split_text(page_content)\n",
        "\n",
        "    # Tag metadata on each chunk\n",
        "    for chunk_num, chunk_text in enumerate(page_chunks, start=1):\n",
        "      chunk_dict = {\n",
        "          \"content\": chunk_text,\n",
        "          \"page\": page_num,\n",
        "          \"chunk_id\": chunk_global_id,\n",
        "          \"chunk_number_on_page\": chunk_num,\n",
        "          \"type\": \"extracted_texts_and_captions_descriptions\"\n",
        "      }\n",
        "      all_chunks.append(chunk_dict)\n",
        "      chunk_global_id += 1\n",
        "\n",
        "  return all_chunks\n",
        "\n",
        "# Preprocess the uploaded PDF\n",
        "def preprocess_pdf(file_path: str, image_dir: str, embedding_model,\n",
        "                   index_file: str = \"index.faiss\",\n",
        "                   chunks_file: str = \"chunks.json\",\n",
        "                   use_cache: bool = True) -> Tuple[faiss.IndexFlatIP, List[Dict]]:\n",
        "\n",
        "  if not os.path.exists(file_path):\n",
        "    raise FileNotFoundError(f\"PDF not found at {file_path}\")\n",
        "\n",
        "  # Loading cache to save on compute time and resources everytime a query is made\n",
        "  if use_cache and os.path.exists(index_file) and os.path.exists(chunks_file):\n",
        "    print(\"[INFO] Loading cached FAISS index and chunks...\")\n",
        "    index = load_faiss_index(index_file)\n",
        "    chunks = load_cache(chunks_file)\n",
        "    return index, chunks\n",
        "\n",
        "  # Cleanup stale cache if not using it or if missing\n",
        "  if not use_cache or not (os.path.exists(index_file) and os.path.exists(chunks_file)):\n",
        "    if os.path.exists(index_file):\n",
        "      os.remove(index_file)\n",
        "    if os.path.exists(chunks_file):\n",
        "      os.remove(chunks_file)\n",
        "\n",
        "  # Otherwise run full processing\n",
        "  try:\n",
        "    page_texts, page_images = extract_pages_text_and_images(file_path, image_dir)\n",
        "  except Exception as e:\n",
        "    print(f\"Error reading PDF: {e}\")\n",
        "    raise e\n",
        "\n",
        "  page_captions = generate_captions_per_page(page_images)\n",
        "  merged_pages = merge_text_and_captions(page_texts, page_captions)\n",
        "\n",
        "  # Delete extracted images after captioning\n",
        "  cleanup_images(image_dir)\n",
        "\n",
        "  # Chunk the merged pages\n",
        "  chunks = chunk_text_with_metadata(merged_pages)\n",
        "  texts = [chunk['content'] for chunk in chunks]\n",
        "\n",
        "  # Generate embeddings and initialize faiss index with the dimensions of the embeddings\n",
        "  embeddings = embedding_model.encode(texts, normalize_embeddings=True)\n",
        "  embeddings = embeddings.astype(np.float32) # Making sure embeddings are in float32 format for FAISS\n",
        "\n",
        "  embedding_dim = embeddings.shape[1]\n",
        "  index = init_faiss_indexflatip(embedding_dim=embedding_dim)\n",
        "\n",
        "  # Add embeddings to index\n",
        "  add_embeddings_to_index(index=index, embeddings=embeddings)\n",
        "\n",
        "  # Save index and chunks\n",
        "  if use_cache:\n",
        "    save_faiss_index(index, index_file)\n",
        "    save_cache(chunks, chunks_file)\n",
        "\n",
        "  return index, chunks\n",
        "\n",
        "# Semantic search funtion that uses preprocessed data\n",
        "def semantic_search(query, embedding_model, index, chunks, top_k=10):\n",
        "  # Embed user query\n",
        "  query_embedding = embedding_model.encode([query], normalize_embeddings=True)\n",
        "\n",
        "  # Retrieve top matches from FAISS\n",
        "  distances, indices = search_faiss_index(index, query_embedding, k=top_k)\n",
        "\n",
        "  # Retrieve matched chunks\n",
        "  retrieved_chunks = [chunks[i] for i in indices[0]]\n",
        "\n",
        "  return retrieved_chunks\n",
        "\n",
        "\n",
        "# Generate answer for Gradio interface\n",
        "def generate_answer_stream(query, retrieved_chunks, model, processor):\n",
        "  \"\"\"Feeds tokens gradually from LLM.\"\"\"\n",
        "  context_texts = [chunk['content'] for chunk in retrieved_chunks]\n",
        "\n",
        "  # Combine system instruction, context, and query into a single string for the user role\n",
        "  system_instruction = \"\"\"You are a helpful and precise assistant for question-answering tasks.\n",
        "                      Use only the following pieces of retrieved context to answer the question.\n",
        "                      You may provide the response in a structured markdown response if necessary.\n",
        "                      If the answer is not found in the provided context, state that the information is not available in the document. Do not use any external knowledge or make assumptions.\n",
        "                      \"\"\"\n",
        "\n",
        "  # Build the core prompt string, excluding specific turn markers\n",
        "  # The processor.apply_chat_template will handle the proper formatting\n",
        "  rag_prompt_content = \"\"\n",
        "  if system_instruction:\n",
        "      rag_prompt_content += f\"{system_instruction.strip()}\\n\\n\"\n",
        "  if context_texts:\n",
        "      rag_prompt_content += \"Context:\\n\" +\"-\"+ \"\\n-\".join(context_texts).strip() + \"\\n\\n\"\n",
        "  rag_prompt_content += f\"Question: {query.strip()}\\nAnswer:\"\n",
        "\n",
        "  # Robust format for multimodal processor\n",
        "  messages = [\n",
        "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": rag_prompt_content}]}\n",
        "    ]\n",
        "\n",
        "  # Prepare model inputs using apply_chat_template\n",
        "  # This will correctly format the prompt for Gemma 3\n",
        "  inputs = processor.apply_chat_template(\n",
        "      messages,\n",
        "      add_generation_prompt=True, # Tell the model it is the start of its turn\n",
        "      tokenize=True,\n",
        "      return_dict=True,\n",
        "      return_tensors=\"pt\",\n",
        "      truncation=True,\n",
        "      max_length=4096  # Apply max_length here if needed, truncation will handle it\n",
        "  ).to(\"cpu\")\n",
        "\n",
        "  streamer = TextIteratorStreamer(processor.tokenizer, skip_prompt=True, decode_kwargs={\"skip_special_tokens\": True})\n",
        "  with torch.inference_mode():\n",
        "    model.generate(**inputs, streamer=streamer, use_cache=True, max_new_tokens=512)\n",
        "    gc.collect() # Free memory after model generation\n",
        "\n",
        "  accumulated = \"\"\n",
        "  for new_text in streamer:\n",
        "    # time.sleep(0.2)\n",
        "    accumulated += new_text\n",
        "    yield accumulated\n",
        "\n",
        "  # Free memory after streaming is complete\n",
        "  clear_gpu_cache()\n",
        "  gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAmw7MN0aLPs",
        "outputId": "db2df7f0-a451-4e45-8bb3-1fc7a986452b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting setup/multimodal_rag/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `app.py`"
      ],
      "metadata": {
        "id": "JmtvZAhzs2Ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile setup/multimodal_rag/app.py\n",
        "\"\"\"Gradio setup for the Multimodal RAG system.\"\"\"\n",
        "import os\n",
        "import torch\n",
        "import shutil\n",
        "import gradio as gr\n",
        "# import gc\n",
        "\n",
        "from utils import save_cache, load_cache, save_faiss_index, load_faiss_index\n",
        "from model_setup import embedding_model, model, processor\n",
        "from main import preprocess_pdf, semantic_search, generate_answer_stream\n",
        "\n",
        "torch.set_num_threads(4)  # cpu thread limit\n",
        "\n",
        "# Creating a cache directory for the retrieved chunks and index files\n",
        "CACHE_DIR = \"cache_dir\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "INDEX_FILE = os.path.join(CACHE_DIR, \"index.faiss\")\n",
        "CHUNKS_FILE = os.path.join(CACHE_DIR, \"chunks.json\")\n",
        "\n",
        "# Global state shared across chats\n",
        "state = {\n",
        "    \"index\": None,\n",
        "    \"chunks\": None,\n",
        "    \"pdf_path\": None,\n",
        "}\n",
        "\n",
        "def handle_pdf_upload(file):\n",
        "  if file is None:\n",
        "      return \"[ERROR] No file uploaded.\"\n",
        "\n",
        "  state[\"pdf_path\"] = file.name\n",
        "  state[\"image_dir\"] = os.path.join(CACHE_DIR, \"extracted_images\")\n",
        "\n",
        "  try:\n",
        "    if os.path.exists(INDEX_FILE) and os.path.exists(CHUNKS_FILE):\n",
        "      # Load from cache\n",
        "      state[\"index\"] = load_faiss_index(INDEX_FILE)\n",
        "      state[\"chunks\"] = load_cache(CHUNKS_FILE)\n",
        "      return \"‚úÖ Loaded from cache and ready for Q&A!\"\n",
        "    else:\n",
        "      # Run your PDF preprocessing\n",
        "      index, chunks = preprocess_pdf(\n",
        "          state[\"pdf_path\"],\n",
        "          state[\"image_dir\"],\n",
        "          embedding_model=embedding_model,\n",
        "          index_file=INDEX_FILE,\n",
        "          chunks_file=CHUNKS_FILE,\n",
        "          use_cache=True)\n",
        "      state[\"index\"] = index\n",
        "      state[\"chunks\"] = chunks\n",
        "\n",
        "      # Save to cache\n",
        "      save_faiss_index(index, INDEX_FILE)\n",
        "      save_cache(chunks, CHUNKS_FILE)\n",
        "\n",
        "      return \"‚úÖ Document processed and ready for Q&A!\"\n",
        "  except Exception as e:\n",
        "    return f\"[‚ö†Ô∏è ERROR] Failed to process document: {e}\"\n",
        "\n",
        "def chat_streaming(message, history):\n",
        "    if state[\"index\"] is None and state[\"chunks\"] is None:\n",
        "      yield \"[ERROR] Please upload and process a PDF first.\"\n",
        "      return\n",
        "\n",
        "    # Perform semantic search\n",
        "    retrieved_chunks = semantic_search(message, embedding_model, state[\"index\"], state[\"chunks\"], top_k=10)\n",
        "\n",
        "    # Stream the answer\n",
        "    for partial in generate_answer_stream(message, retrieved_chunks, model, processor):\n",
        "      yield partial\n",
        "\n",
        "# Function for clearing the cache files before uploading another document to prevent stale cache retrieval\n",
        "def manual_clear_cache():\n",
        "  if not os.path.exists(INDEX_FILE) or not os.path.exists(CHUNKS_FILE):\n",
        "    return \"‚ö†Ô∏èNo cache files exists to clear.\"\n",
        "  if os.path.exists(CACHE_DIR):\n",
        "    shutil.rmtree(CACHE_DIR)\n",
        "\n",
        "  state[\"index\"], state[\"chunks\"] = None, None\n",
        "  return \"‚úÖ Cache cleared! You can upload a new document now.\"\n",
        "\n",
        "description = \"\"\"\n",
        " Remember to be specific when querying for better response.\n",
        " üìñüßê\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## üìöMultimodal RAG System\\nUpload a PDF (‚â§50 pages recommended) and ask questions about it.\")\n",
        "\n",
        "    with gr.Row():\n",
        "      file_input = gr.File(label=\"üìÇUpload PDF\")\n",
        "      upload_button = gr.Button(\"üîÅProcess PDF\")\n",
        "\n",
        "    with gr.Row():\n",
        "      clear_cache_button = gr.Button(\"üßπ Clear Cache\")\n",
        "      clear_cache_status = gr.Textbox(label=\"Cache Clear Status\", interactive=False)\n",
        "\n",
        "    upload_status = gr.Textbox(label=\"Upload Status\", interactive=False)\n",
        "    upload_button.click(handle_pdf_upload, inputs=file_input, outputs=upload_status)\n",
        "    clear_cache_button.click(manual_clear_cache, outputs=clear_cache_status)\n",
        "\n",
        "    chat = gr.ChatInterface(\n",
        "            fn=chat_streaming,\n",
        "            type=\"messages\",\n",
        "            title=\"üìÑAsk Questions from PDF\",\n",
        "            description=description,\n",
        "            examples=[[\"What is this document about?\"]]\n",
        "        )\n",
        "    chat.queue()\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXNMtr5ra94G",
        "outputId": "05372599-0939-4666-9197-a2844a239971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting setup/multimodal_rag/app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `README.md`"
      ],
      "metadata": {
        "id": "F9a6o6WX_XwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile setup/multimodal_rag/README.md\n",
        "---\n",
        "title: Multimodal RAG System üìÑ\n",
        "emoji: üìö\n",
        "colorFrom: purple\n",
        "colorTo: pink\n",
        "sdk: gradio\n",
        "sdk_version: 5.43.1\n",
        "app_file: app.py\n",
        "pinned: false\n",
        "license: mit\n",
        "---\n",
        "\n",
        "# Multimodal RAG System üìñ\n",
        "\n",
        "A **Multimodal Retrieval-Augmented Generation (RAG) system** that allows users to upload PDFs and ask questions based on the text, images, and tables in the document. Uses **Gemma3** for image captioning and multimodal text generation and **SentenceTransformer** + **FAISS** for semantic search.\n",
        "\n",
        "## Features\n",
        "\n",
        "- Extracts text and images from PDF documents.\n",
        "- Generates factual captions for images and tables.\n",
        "- Chunks the combined text + captions for efficient retrieval.\n",
        "- Stores embeddings in a FAISS index for fast semantic search.\n",
        "- Streams answers from the LLM using Gradio interface.\n",
        "- Efficient memory usage with bitsandbytes 4-bit quantization.\n",
        "\n",
        "The **[google/gemma-3-4b-it](https://huggingface.co/google/gemma-3-4b-it)** is both used to generate image descriptions for the extracted images and for text generation for the RAG system."
      ],
      "metadata": {
        "id": "TnlMQhDE_RmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `requirements.txt`"
      ],
      "metadata": {
        "id": "XfGmvtBnIrZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile setup/multimodal_rag/requirements.txt\n",
        "torch\n",
        "transformers\n",
        "numpy\n",
        "gradio\n",
        "pillow\n",
        "PyMuPDF\n",
        "faiss-cpu\n",
        "sentence-transformers\n",
        "langchain\n",
        "bitsandbytes\n",
        "accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwF1c0uZ_Raq",
        "outputId": "781c4bf8-b15d-435c-db8a-b4895df16fbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting setup/multimodal_rag/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls setup/multimodal_rag/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9ECNPJo4qP-",
        "outputId": "cb532271-5fe0-4c83-93e5-43a6cdbf0770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\tmain.py  model_setup.py  README.md  requirements.txt  utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading to Hugging Face Hub Space"
      ],
      "metadata": {
        "id": "k7NsvYXcunNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"huggingface_hub\")"
      ],
      "metadata": {
        "id": "oD4TXS4oE0dC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo, get_full_repo_name, upload_file, upload_folder\n",
        "\n",
        "# Defining the parameters to use for the upload\n",
        "LOCAL_FOLDER_TO_UPLOAD = \"setup/multimodal_rag\"\n",
        "HF_TARGET_SPACE_NAME = \"multimodal_rag_system\"\n",
        "HF_REPO_TYPE = \"space\"\n",
        "HF_SPACE_SDK = \"gradio\"\n",
        "# HF_TOKEN = \"\"\n",
        "\n",
        "# Create a space repo on Hugging Face Hub\n",
        "print(f\"‚ÑπÔ∏è Creating repo on HF Hub with name: {HF_TARGET_SPACE_NAME}\")\n",
        "create_repo(\n",
        "    repo_id=HF_TARGET_SPACE_NAME,\n",
        "    # token=HF_TOKEN,\n",
        "    repo_type=HF_REPO_TYPE,\n",
        "    private=False,\n",
        "    space_sdk=HF_SPACE_SDK,\n",
        "    exist_ok=True, # Prevent errors when same repo is re-uploaded\n",
        ")\n",
        "\n",
        "# Get the full repo name\n",
        "full_hf_repo_name = get_full_repo_name(model_id=HF_TARGET_SPACE_NAME)\n",
        "print(f\"‚ÑπÔ∏è Full Hugging Face Hub repo name: {full_hf_repo_name}\")\n",
        "\n",
        "# Upload the folder\n",
        "print(f\"‚ÑπÔ∏è Uploading {LOCAL_FOLDER_TO_UPLOAD} to repo name: {full_hf_repo_name}\")\n",
        "folder_upload_url = upload_folder(\n",
        "    repo_id=full_hf_repo_name,\n",
        "    folder_path=LOCAL_FOLDER_TO_UPLOAD,\n",
        "    path_in_repo=\".\", # upload to the root directory\n",
        "    repo_type=HF_REPO_TYPE,\n",
        "    commit_message=\"Uploading Mulitimodal Retrieval Augmented Generation System.\"\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Folder succesfully uploaded with commit URL: {folder_upload_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxjEd58_uUdx",
        "outputId": "a6ee00ad-4a8a-4802-eeba-053f2e8c5fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ÑπÔ∏è Creating repo on HF Hub with name: multimodal_rag_system\n",
            "‚ÑπÔ∏è Full Hugging Face Hub repo name: Saint5/multimodal_rag_system\n",
            "‚ÑπÔ∏è Uploading setup/multimodal_rag to repo name: Saint5/multimodal_rag_system\n",
            "‚úÖ Folder succesfully uploaded with commit URL: https://huggingface.co/spaces/Saint5/multimodal_rag_system/tree/main/.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ro3BaeLfSBsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # %%writefile setup/multimodal_rag/app.py\n",
        "# \"\"\"Gradio setup for the Multimodal RAG system.\"\"\"\n",
        "# import os\n",
        "# import torch\n",
        "# import gradio as gr\n",
        "# # import gc\n",
        "\n",
        "# # from utils import load_faiss_index, load_cache\n",
        "# # from model_setup import embedding_model, model, processor\n",
        "# # from main import preprocess_pdf, semantic_search, generate_answer_stream\n",
        "\n",
        "# # torch.set_num_threads(4)  # cpu thread limit\n",
        "\n",
        "# CACHE_DIR = \"cache\"\n",
        "# os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# INDEX_FILE = os.path.join(CACHE_DIR, \"index.faiss\")\n",
        "# CHUNKS_FILE = os.path.join(CACHE_DIR, \"chunks.json\")\n",
        "\n",
        "# # Global state shared across chats\n",
        "# state = {\n",
        "#     \"index\": None,\n",
        "#     \"chunks\": None,\n",
        "#     \"pdf_path\": None,\n",
        "#     \"image_dir\": \"extracted_images\",\n",
        "# }\n",
        "\n",
        "# # Function to clear cache to prevent stale cache retrieval if new document is uploaded\n",
        "# def clear_cache_files():\n",
        "#   if os.path.exists(INDEX_FILE):\n",
        "#       os.remove(INDEX_FILE)\n",
        "#   if os.path.exists(CHUNKS_FILE):\n",
        "#       os.remove(CHUNKS_FILE)\n",
        "#   state[\"index\"], state[\"chunks\"] = None, None\n",
        "\n",
        "# def handle_pdf_upload(file):\n",
        "#   if file is None:\n",
        "#       return \"[ERROR ‚ö†Ô∏è] No file uploaded.\"\n",
        "\n",
        "#   # Save uploaded file to cache directory to ensure accessibility\n",
        "#   pdf_path = os.path.join(CACHE_DIR, os.path.basename(file.name))\n",
        "#   with open(pdf_path, \"wb\") as f_out:\n",
        "#       f_out.write(file.file.read())\n",
        "\n",
        "#   if state[\"pdf_path\"] != pdf_path:\n",
        "#       clear_cache_files()\n",
        "\n",
        "#   state[\"pdf_path\"] = pdf_path\n",
        "\n",
        "#   index, chunks = preprocess_pdf(\n",
        "#       file_path=state[\"pdf_path\"],\n",
        "#       image_dir=state[\"image_dir\"],\n",
        "#       embedding_model=embedding_model,\n",
        "#       index_file=INDEX_FILE,\n",
        "#       chunks_file=CHUNKS_FILE,\n",
        "#       use_cache=True\n",
        "#   )\n",
        "#   state[\"index\"], state[\"chunks\"] = index, chunks\n",
        "#   return \"‚úÖ Document processed and ready for Q&A!\"\n",
        "\n",
        "# def chat_streaming(message, history):\n",
        "#   if state[\"index\"] is None or state[\"chunks\"] is None:\n",
        "#     yield \"[ERROR ‚ö†Ô∏è] Please upload and process a PDF first.\"\n",
        "#     return\n",
        "#   retrieved_chunks = semantic_search(message, embedding_model, state[\"index\"], state[\"chunks\"], top_k=10)\n",
        "#   for partial in generate_answer_stream(message, retrieved_chunks, model, processor):\n",
        "#     yield partial\n",
        "\n",
        "\n",
        "# description = \"\"\"\n",
        "# Remember to be specific when querying for better response.\n",
        "# üìñüßê\n",
        "# \"\"\"\n",
        "# # Gradio setup\n",
        "# with gr.Blocks() as demo:\n",
        "#   gr.Markdown(\"\"\"## üìöMultimodal RAG System\n",
        "#                 Upload a PDF (‚â§50 pages recommended) and ask questions about it.\"\"\")\n",
        "#   with gr.Row():\n",
        "#     file_input = gr.File(label=\"üìÇUpload PDF\")\n",
        "#     upload_button = gr.Button(\"üîÅProcess PDF\")\n",
        "#   upload_status = gr.Textbox(label=\"Upload Status\", interactive=False)\n",
        "\n",
        "#   upload_button.click(handle_pdf_upload, inputs=file_input, outputs=upload_status)\n",
        "\n",
        "#   chat = gr.ChatInterface(\n",
        "#       fn=chat_streaming,\n",
        "#       type=\"messages\",\n",
        "#       title=\"üìÑüòÉ Ask Questions on your PDF!\",\n",
        "#       description=description,\n",
        "#       examples=[[\"What is this document about?\"]]\n",
        "#   )\n",
        "#   chat.queue()\n",
        "\n",
        "# demo.launch()"
      ],
      "metadata": {
        "id": "IPUC-bLX-R6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # %%writefile setup/multimodal_rag/app.py\n",
        "\n",
        "# \"\"\"Gradio setup for the Multimodal RAG system.\"\"\"\n",
        "\n",
        "# import os\n",
        "# import hashlib\n",
        "# import torch\n",
        "# import gradio as gr\n",
        "# # import gc\n",
        "\n",
        "# from utils import load_faiss_index, load_cache\n",
        "# from model_setup import embedding_model, model, processor\n",
        "# from main import preprocess_pdf, semantic_search, generate_answer_stream\n",
        "\n",
        "# torch.set_num_threads(4) # Limits to 4 threads for better performance\n",
        "\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# # Ensure cache directory exists\n",
        "# CACHE_DIR = \"cache\"\n",
        "# os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# # Global state shared across chats\n",
        "# state = {\n",
        "#     \"index\": None,\n",
        "#     \"chunks\": None,\n",
        "#     \"pdf_path\": None,\n",
        "#     \"image_dir\": \"extracted_images\",  # Default image directory\n",
        "#     \"index_file\": None,\n",
        "#     \"chunks_file\": None,\n",
        "#     \"processed_pdfs\": {}, # pdf_name -> (index_file, chunks_file)\n",
        "# }\n",
        "\n",
        "# def _make_cache_names(pdf_path: str) -> tuple[str, str]:\n",
        "#     \"\"\"Generate unique cache file names per PDF based on hash of filename.\"\"\"\n",
        "#     pdf_hash = hashlib.md5(pdf_path.encode()).hexdigest()[:8]  # Shorten for readability\n",
        "#     base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
        "#     index_file = os.path.join(CACHE_DIR, f\"{base_name}_{pdf_hash}_index.faiss\")\n",
        "#     chunks_file = os.path.join(CACHE_DIR, f\"{base_name}_{pdf_hash}_chunks.json\")\n",
        "#     return index_file, chunks_file\n",
        "\n",
        "# def handle_pdf_upload(file):\n",
        "#     if file is None:\n",
        "#         return \"[ERROR ‚ö†Ô∏è] No file uploaded.\", gr.update()\n",
        "\n",
        "#     # Save uploaded file to cache directory to ensure accessibility\n",
        "#     new_pdf_path = os.path.join(CACHE_DIR, os.path.basename(file.name))\n",
        "#     with open(new_pdf_path, \"wb\") as f_out:\n",
        "#       f_out.write(file.file.read())\n",
        "#     state[\"pdf_path\"] = new_pdf_path\n",
        "\n",
        "#     # Create unique cache file names for this PDF\n",
        "#     state[\"index_file\"], state[\"chunks_file\"] = _make_cache_names(new_pdf_path)\n",
        "\n",
        "#     # Run preprocessing (reuse cache if exists)\n",
        "#     index, chunks = preprocess_pdf(\n",
        "#         file_path=state[\"pdf_path\"],\n",
        "#         image_dir=state[\"image_dir\"],\n",
        "#         embedding_model=embedding_model,\n",
        "#         index_file=state[\"index_file\"],\n",
        "#         chunks_file=state[\"chunks_file\"],\n",
        "#         use_cache=True # allow cache for the PDF\n",
        "#     )\n",
        "#     state[\"index\"], state[\"chunks\"] = index, chunks\n",
        "#     # gc.collect() # Free memeory after PDF processing\n",
        "\n",
        "#     # Store in processed_pdfs for later selection\n",
        "#     pdf_key = os.path.basename(new_pdf_path) # Use PDF basename as dropdown key and store full cache paths as value\n",
        "#     state[\"processed_pdfs\"][pdf_key] = (state[\"index_file\"], state[\"chunks_file\"])\n",
        "\n",
        "#     return (\n",
        "#         f\"‚úÖ Document '{pdf_key}' processed and ready for Q&A!\",\n",
        "#         gr.update(choices=list(state[\"processed_pdfs\"].keys()), value=pdf_key)\n",
        "#     )\n",
        "\n",
        "# def handle_pdf_selection(pdf_name):\n",
        "#     \"\"\"Switch active PDF from dropdown.\"\"\"\n",
        "#     if pdf_name not in state[\"processed_pdfs\"]:\n",
        "#         return \"[ERROR] Selected PDF not found in cache.\"\n",
        "\n",
        "#     # state[\"pdf_path\"] = os.path.join(CACHE_DIR, pdf_name)\n",
        "#     # state[\"index_file\"], state[\"chunks_file\"] = state[\"processed_pdfs\"][pdf_name]\n",
        "\n",
        "#     # Retrieve cached full paths\n",
        "#     state[\"index_file\"], state[\"chunks_file\"] = state[\"processed_pdfs\"][pdf_name]\n",
        "\n",
        "#     # Optionally reset pdf_path or keep as None if not needed\n",
        "#     state[\"pdf_path\"] = None  # Or remove if not used after upload\n",
        "\n",
        "#     # Reload index + chunks from cache directly\n",
        "#     index = load_faiss_index(state[\"index_file\"])\n",
        "#     chunks = load_cache(state[\"chunks_file\"])\n",
        "\n",
        "#     state[\"index\"], state[\"chunks\"] = index, chunks\n",
        "#     return f\"üìÇ Switched to cached PDF: {pdf_name}\"\n",
        "\n",
        "# def chat_streaming(message, history):\n",
        "#     if state[\"index\"] is None or state[\"chunks\"] is None:\n",
        "#         yield \"[ERROR ‚ö†Ô∏è] Please upload and process a PDF first.\"\n",
        "#         return\n",
        "\n",
        "#     # Perform semantic search\n",
        "#     retrieved_chunks = semantic_search(message, embedding_model, state[\"index\"], state[\"chunks\"], top_k=10)\n",
        "#     # gc.collect() # Free memory after semantic search\n",
        "\n",
        "#     # Stream the answer\n",
        "#     for partial in generate_answer_stream(message, retrieved_chunks, model, processor):\n",
        "#         yield partial\n",
        "\n",
        "# description = \"\"\"\n",
        "#  Remember to be specific when querying for better response.\n",
        "#  üìñüßê\n",
        "# \"\"\"\n",
        "\n",
        "# with gr.Blocks() as demo:\n",
        "#     gr.Markdown(\"\"\"\n",
        "#                 ## üìöSimple Multimodal RAG System\n",
        "#                 Upload a PDF (‚â§50 pages recommended) and ask questions about it.\n",
        "#                 Supports multiple PDFs, just upload and select from the dropdown.\"\"\")\n",
        "\n",
        "#     with gr.Row():\n",
        "#         file_input = gr.File(label=\"üìÇUpload PDF\")\n",
        "#         # upload_button = gr.Button(\"Process PDF\")\n",
        "\n",
        "#     upload_status = gr.Textbox(label=\"Upload Status\", interactive=False)\n",
        "#     pdf_selector = gr.Dropdown(label=\"üìÑ Select a Processed PDF\", choices=[], interactive=True)\n",
        "\n",
        "#     upload_button = gr.Button(\"Process PDF\")\n",
        "#     upload_button.click(handle_pdf_upload, inputs=file_input, outputs=[upload_status, pdf_selector])\n",
        "\n",
        "#     # Switch active PDF from dropdown\n",
        "#     pdf_selector.change(handle_pdf_selection, inputs=pdf_selector, outputs=upload_status)\n",
        "\n",
        "#     chat = gr.ChatInterface(\n",
        "#             fn=chat_streaming,\n",
        "#             type=\"messages\",\n",
        "#             title=\"üìÑüòÉ Ask Questions from PDF\",\n",
        "#             description=description,\n",
        "#             examples=[[\"What is this document about?\"]]\n",
        "#         )\n",
        "#     chat.queue()\n",
        "\n",
        "# demo.launch()"
      ],
      "metadata": {
        "id": "9FakMhuDs448"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x1Uf1f15xNWS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}